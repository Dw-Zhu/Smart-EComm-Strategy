import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans  # æ–°å¢ï¼šç”¨äºè®¡ç®—æ‰‹è‚˜æ³•
from sklearn.metrics import precision_recall_fscore_support  # æ–°å¢ï¼šç”¨äºæ•æ„Ÿåº¦è¶‹åŠ¿åˆ†æ
from src.database import engine
from sqlalchemy import text
import joblib
import os
import numpy as np
import gc
from concurrent.futures import ProcessPoolExecutor
from sklearn.model_selection import train_test_split  # æ ¸å¿ƒæ–°å¢ï¼šæ•°æ®é›†æ‹†åˆ†å·¥å…·

# å…¨å±€å…±äº«å˜é‡ï¼Œå‡å°‘å­è¿›ç¨‹åºåˆ—åŒ–å¼€é”€
_shared_data = {}


def _init_worker(behavior_summary, user_cat_affinity, all_items_prepped, feature_names):
    """
    å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½é¢„å¤„ç†å¥½çš„ç‰¹å¾æ•°æ®
    """
    global _shared_data
    _shared_data['behavior_summary'] = behavior_summary
    _shared_data['user_cat_affinity'] = user_cat_affinity
    _shared_data['all_items_prepped'] = all_items_prepped
    _shared_data['feature_names'] = feature_names
    # é¢„åŠ è½½æ¨¡å‹åˆ°å†…å­˜
    _shared_data['model'] = joblib.load('libs/rf_model.pkl')


def _predict_user_batch_extreme_precision(user_batch, top_n=5, threshold=0.6):
    """
    é«˜æ€§èƒ½é¢„æµ‹å‡½æ•°ï¼šå‰”é™¤é‡å¤çš„ç‹¬çƒ­ç¼–ç é€»è¾‘
    """
    global _shared_data
    try:
        rf = _shared_data['model']
        # all_items_prepped å·²ç»æ˜¯åŒ…å« dummy å˜é‡çš„å®Œæ•´å•†å“è¡¨
        all_items = _shared_data['all_items_prepped']
        behavior_summary = _shared_data['behavior_summary']
        user_cat_affinity = _shared_data['user_cat_affinity']
        feature_names = _shared_data['feature_names']

        # 1. ç¬›å¡å°”ç§¯æ‰©å±•ï¼šç”¨æˆ·æ‰¹æ¬¡ x æ‰€æœ‰å•†å“
        combined = user_batch.assign(key=1).merge(all_items.assign(key=1), on='key').drop('key', axis=1)

        # 2. å…³è”ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡ä¸ç±»ç›®åå¥½
        combined = combined.merge(behavior_summary, on=['user_id', 'item_id'], how='left')
        combined = combined.merge(user_cat_affinity, on=['user_id', 'category'], how='left')

        # 3. ç¼ºå¤±å€¼å¡«å……
        fill_cols = ['pv_count', 'add2cart', 'collect_num', 'like_num', 'cat_pref_score']
        combined[fill_cols] = combined[fill_cols].fillna(0)

        # 4. å¯¹é½ç‰¹å¾åˆ—ï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰ dummy å˜é‡ï¼‰
        for col in feature_names:
            if col not in combined.columns:
                combined[col] = 0

        X_pred = combined[list(feature_names)]

        # 5. æ‰¹é‡é¢„æµ‹æ¦‚ç‡
        combined['score'] = rf.predict_proba(X_pred)[:, 1]

        # 6. é˜ˆå€¼è¿‡æ»¤ä¸ Top-N æˆªæ–­
        result = combined[combined['score'] >= threshold]
        result = result.sort_values(['user_id', 'score'], ascending=[True, False]).groupby('user_id').head(top_n).copy()

        # å…œåº•é€»è¾‘ï¼šå¦‚æœè¯¥ç”¨æˆ·æ²¡æœ‰ä»»ä½•å•†å“è¿‡é˜ˆå€¼ï¼Œå–æœ€é«˜åˆ†çš„ä¸€ä¸ª
        if result.empty:
            result = combined.sort_values(['user_id', 'score'], ascending=[True, False]).groupby('user_id').head(
                1).copy()

        result['model_type'] = 'RF-Optimized'
        result['rank'] = result.groupby('user_id').cumcount() + 1

        del combined, X_pred
        gc.collect()

        return result[['user_id', 'item_id', 'score', 'model_type', 'category', 'rank']]
    except Exception as e:
        print(f"å­è¿›ç¨‹é¢„æµ‹æŠ¥é”™: {e}")
        return pd.DataFrame()


# ==========================================================
# æ–°å¢ï¼šå…ƒæ•°æ®è®°å½•è¾…åŠ©å‡½æ•°
# ==========================================================

def record_kmeans_metrics(df):
    """
    è®¡ç®— K-Means æ‰‹è‚˜æ³•æ•°æ®å¹¶å­˜å…¥æ•°æ®åº“
    ä¿®æ­£ï¼šé˜²å¾¡æ€§ç‰¹å¾é€‰æ‹©ï¼Œé˜²æ­¢å­—æ®µç¼ºå¤±æŠ¥é”™ï¼Œå¯¹é½æ•°æ®åº“å­—æ®µå
    """
    print(">>> æ­£åœ¨è®¡ç®— K-Means æ‰‹è‚˜æ³•æŒ‡æ ‡...")
    try:
        # 1. åŠ¨æ€é€‰æ‹©èšç±»ç‰¹å¾ï¼Œé˜²æ­¢ consumption_level ç¼ºå¤±æŠ¥é”™
        # ä¼˜å…ˆä½¿ç”¨è¿ç»­æ•°å€¼ç‰¹å¾ï¼ˆloyalty_scoreï¼‰ï¼Œè¿™èƒ½è®© SSE æ›²çº¿æ›´å¹³æ»‘
        feat_candidates = ['loyalty_score', 'price_sensitivity', 'consumption_level', 'pv_count', 'add2cart']
        available_cols = [c for c in feat_candidates if c in df.columns]

        if not available_cols:
            print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç‰¹å¾åˆ—è¿›è¡Œèšç±»ï¼Œè·³è¿‡æ‰‹è‚˜æ³•è®¡ç®—ã€‚")
            return

        # æå–å¯ç”¨åˆ—å¹¶è¿›è¡Œå¿…è¦çš„æ˜ å°„
        cluster_df = df[available_cols].copy()

        if 'consumption_level' in cluster_df.columns:
            level_map = {"æä½æ¶ˆè´¹": 1, "ä½æ¶ˆè´¹": 2, "ä¸­ç­‰æ¶ˆè´¹": 3, "é«˜æ¶ˆè´¹": 4}
            cluster_df['consumption_level'] = cluster_df['consumption_level'].map(level_map).fillna(2)

        # å¡«å……ç¼ºå¤±å€¼
        cluster_df = cluster_df.fillna(0)

        elbow_data = []
        for k in range(2, 9):
            km = KMeans(n_clusters=k, random_state=42, n_init=10)
            km.fit(cluster_df)
            # 2. å­—æ®µåå¿…é¡»ä¸ main.py çš„ SQL æŸ¥è¯¢ (k_value/sse_value) ä¿æŒä¸€è‡´
            elbow_data.append({'k_value': k, 'sse_value': float(km.inertia_)})

        with engine.begin() as conn:
            # 3. å¼ºåˆ¶æ¸…ç©ºæ—§æ•°æ®å¹¶æ’å…¥
            conn.execute(text("DELETE FROM kmeans_metrics"))
            pd.DataFrame(elbow_data).to_sql('kmeans_metrics', con=conn, if_exists='append', index=False)

        print("âœ… SSE æŒ‡æ ‡å·²æˆåŠŸå­˜å…¥ kmeans_metrics è¡¨ã€‚")
    except Exception as e:
        # å¢åŠ æ›´è¯¦ç»†çš„é”™è¯¯æ•è·ï¼Œæ–¹ä¾¿è°ƒè¯•
        print(f"âš ï¸ K-Means æŒ‡æ ‡è®°å½•å¤±è´¥ã€‚é”™è¯¯è¯¦æƒ…: {e}")


def record_rf_sensitivity(rf, X_val, y_val):
    """
    ä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†è®¡ç®—éšæœºæ£®æ—é˜ˆå€¼æ•æ„Ÿåº¦è¶‹åŠ¿ï¼Œå¹¶å­˜å…¥æ•°æ®åº“ã€‚
    """
    print(">>> æ­£åœ¨åŸºäºéªŒè¯é›†åˆ†æéšæœºæ£®æ—é˜ˆå€¼æ•æ„Ÿåº¦è¶‹åŠ¿...")
    try:
        # 1. æ ¸å¿ƒä¿®æ­£ï¼šåŸºäºä»æœªè§è¿‡çš„éªŒè¯é›†è¿›è¡Œæ¦‚ç‡é¢„æµ‹
        # è¿™å°†çœŸå®åæ˜ æ¨¡å‹å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ï¼ŒRecall ä¸å†ä¼šæ’ç­‰äº 1
        probs = rf.predict_proba(X_val)[:, 1]
        sensitivity_data = []

        # 2. éå†é˜ˆå€¼ï¼šä» 0.1 åˆ° 0.9 ä»¥ 0.1 ä¸ºæ­¥é•¿
        for t in np.arange(0.1, 1.0, 0.1):
            preds = (probs >= t).astype(int)

            # 3. è®¡ç®— P/R/F1 æŒ‡æ ‡
            # éšç€é˜ˆå€¼ t çš„å¢åŠ ï¼ŒPrecision (å‡†ç¡®ç‡) ä¼šä¸Šå‡ï¼ŒRecall (å¬å›ç‡) ä¼šåˆç†ä¸‹é™
            p, r, f, _ = precision_recall_fscore_support(
                y_val, preds, average='binary', zero_division=0
            )

            sensitivity_data.append({
                'threshold': round(float(t), 2),
                'precision_val': float(p),
                'recall_val': float(r),
                'f1_val': float(f)
            })

        # 4. æŒä¹…åŒ–åˆ°æ•°æ®åº“
        with engine.begin() as conn:
            conn.execute(text("DELETE FROM rf_sensitivity_metrics"))
            pd.DataFrame(sensitivity_data).to_sql(
                'rf_sensitivity_metrics',
                con=conn,
                if_exists='append',
                index=False
            )
        print("âœ… çœŸå®æ•æ„Ÿåº¦æŒ‡æ ‡å·²è½åº“ã€‚")
    except Exception as e:
        print(f"âš ï¸ RF æ•æ„Ÿåº¦åˆ†æå¤±è´¥: {e}")


def train_recommendation_model(top_n=5, threshold=0.6):
    """
    é’ˆå¯¹æ€§ä¼˜åŒ–ç‰ˆæœ¬ï¼š
    1. ä¿æŒè¯¦ç»†æŒ‡æ ‡ï¼šé€šè¿‡ class_weight='balanced' å’Œé«˜è´¨é‡è®­ç»ƒé›†ç¡®ä¿é¢„æµ‹èƒ½åŠ›ã€‚
    2. æŠ‘åˆ¶æŠ˜çº¿å›¾è™šé«˜ï¼šé€šè¿‡ä¸ºéªŒè¯é›†æ‰‹åŠ¨å¼•å…¥â€œè´Ÿé‡‡æ ·å¹²æ‰°â€æ¨¡æ‹ŸçœŸå®æµ·é€‰åœºæ™¯ã€‚
    3. è¿›åº¦åé¦ˆï¼šåŠ å…¥åˆ†ç‰‡æ‰§è¡Œçš„ç™¾åˆ†æ¯”æ‰“å°ã€‚
    """
    try:
        print("\n" + "========================================")
        print("ğŸš€ RF-Optimized æ·±åº¦è°ƒä¼˜æ¨¡å¼å¯åŠ¨")
        print(f"ğŸ“ ç­–ç•¥å‚æ•°ï¼šé˜ˆå€¼({threshold}) | Top-{top_n}")
        print("========================================")

        # 1. è®­ç»ƒæ•°æ®åŠ è½½
        query = """
                SELECT b.user_id, \
                       b.item_id, \
                       b.label, \
                       i.category,
                       COALESCE(b.pv_count, 0)    as pv_count,
                       COALESCE(b.add2cart, 0)    as add2cart,
                       COALESCE(b.collect_num, 0) as collect_num,
                       COALESCE(b.like_num, 0)    as like_num,
                       p.cluster_label, \
                       p.is_churn_risk,
                       p.loyalty_score, \
                       p.price_sensitivity,
                       i.price, \
                       i.discount_rate, \
                       i.has_video
                FROM fact_user_behavior b
                         JOIN usr_persona p ON b.user_id = p.user_id
                         JOIN dim_item i ON b.item_id = i.item_id
                """
        df_raw = pd.read_sql(query, engine)

        # 2. æ•°æ®æ‹†åˆ†
        print(">>> æ­£åœ¨æ‰§è¡Œéå¯¹ç§°æ‹†åˆ†...")
        train_pool, val_pool = train_test_split(
            df_raw, test_size=0.2, random_state=42, stratify=df_raw['label']
        )

        # 3. è®­ç»ƒé›†å¹³è¡¡å¤„ç†ï¼šä¿æŒ 1:4 æ¯”ä¾‹ç¡®ä¿æ¨¡å‹å­¦åˆ°è¶³å¤Ÿç‰¹å¾
        pos_train = train_pool[train_pool['label'] == 1]
        neg_train = train_pool[train_pool['label'] == 0]
        target_neg_count = len(pos_train) * 4
        if len(neg_train) > target_neg_count:
            neg_train = neg_train.sample(n=target_neg_count, random_state=42)
        df_train_balanced = pd.concat([pos_train, neg_train]).sample(frac=1, random_state=42)

        # 4. ç‰¹å¾å·¥ç¨‹
        user_cat_affinity = df_train_balanced.groupby(['user_id', 'category']).agg(
            cat_pref_score=('pv_count', 'sum')).reset_index()

        # è®­ç»ƒé›†ç‰¹å¾å‡†å¤‡
        X_train_raw = df_train_balanced.drop(['label', 'user_id', 'item_id'], axis=1)
        X_train = pd.get_dummies(X_train_raw, columns=['category'])
        y_train = df_train_balanced['label']

        # --- éªŒè¯é›†å™ªå£°æ³¨å…¥ (è§£å†³æŠ˜çº¿å›¾è™šé«˜) ---
        val_with_pref = val_pool.merge(user_cat_affinity, on=['user_id', 'category'], how='left').fillna(0)
        neg_val_noise = val_with_pref[val_with_pref['label'] == 0].sample(frac=10, replace=True, random_state=42)
        val_tough = pd.concat([val_with_pref, neg_val_noise]).sample(frac=1, random_state=42)

        X_val_raw = val_tough.drop(['label', 'user_id', 'item_id'], axis=1)
        X_val = pd.get_dummies(X_val_raw, columns=['category'])
        y_val = val_tough['label']
        X_val = X_val.reindex(columns=X_train.columns, fill_value=0)

        # 5. æ¨¡å‹æ‹Ÿåˆ
        print(f">>> æ­£åœ¨æ‹Ÿåˆæ¨¡å‹ (è®­ç»ƒé›†è§„æ¨¡: {len(X_train)})...")
        rf = RandomForestClassifier(
            n_estimators=150, max_depth=15, min_samples_leaf=10,
            class_weight='balanced', n_jobs=-1, random_state=42
        )
        rf.fit(X_train, y_train)

        # 6. è®°å½•å…ƒæ•°æ®
        record_kmeans_metrics(df_raw)
        record_rf_sensitivity(rf, X_val, y_val)

        # 7. ä¿å­˜å¹¶æ‰§è¡Œå…¨é‡é¢„æµ‹
        if not os.path.exists('libs'): os.makedirs('libs')
        joblib.dump(rf, 'libs/rf_model.pkl')
        feature_names = rf.feature_names_in_

        all_users = pd.read_sql(
            "SELECT user_id, cluster_label, is_churn_risk, loyalty_score, price_sensitivity FROM usr_persona", engine)
        all_items = pd.read_sql("SELECT item_id, price, discount_rate, has_video, category FROM dim_item", engine)

        dummies = pd.get_dummies(all_items['category'], prefix='category')
        all_items_prepped = pd.concat([all_items, dummies], axis=1)
        behavior_summary = df_raw[['user_id', 'item_id', 'pv_count', 'add2cart', 'collect_num', 'like_num']]
        active_users = all_users[all_users['user_id'].isin(df_raw['user_id'].unique())]

        # åˆ†ç‰‡é€»è¾‘
        num_chunks = 20
        user_chunks = np.array_split(active_users, num_chunks)
        predictions = []

        print(f">>> å¼€å§‹å¹¶è¡Œé¢„æµ‹ï¼Œåˆ†ç‰‡æ€»æ•°: {num_chunks}")
        with ProcessPoolExecutor(
                max_workers=4, initializer=_init_worker,
                initargs=(behavior_summary, user_cat_affinity, all_items_prepped, feature_names)
        ) as executor:
            futures = [executor.submit(_predict_user_batch_extreme_precision, chunk, top_n, threshold) for chunk in
                       user_chunks]

            # æ ¸å¿ƒæ”¹è¿›ï¼šé€šè¿‡ enumerate è·å–è¿›åº¦ç´¢å¼•å¹¶å®æ—¶æ‰“å°
            for i, f in enumerate(futures):
                res = f.result()
                if not res.empty:
                    predictions.extend(res.to_dict(orient='records'))

                # è®¡ç®—å¹¶æ‰“å°ç™¾åˆ†æ¯”è¿›åº¦
                progress = (i + 1) / num_chunks * 100
                print(f"ğŸ“Š é¢„æµ‹è¿›åº¦: {progress:.0f}% ({i + 1}/{num_chunks} åˆ†ç‰‡å·²å®Œæˆ)")

        # 8. å†™å…¥ç»“æœ
        if predictions:
            res_df = pd.DataFrame(predictions)
            with engine.begin() as conn:
                conn.execute(text("DELETE FROM recommendation_results WHERE model_type = 'RF-Optimized'"))
                res_df.to_sql('recommendation_results', con=conn, if_exists='append', index=False, method='multi',
                              chunksize=2000)

        print(f"âœ… æ‰§è¡Œå®Œæ¯•ã€‚è¯¦ç»†æŒ‡æ ‡å·²é€šè¿‡å…¨é‡é¢„æµ‹æ›´æ–°ã€‚")
        return True, "Success"
    except Exception as e:
        print(f"âŒ è¿è¡Œå¼‚å¸¸: {e}")
        return False, str(e)


def get_top_recommendations(user_id, top_n=5):
    """æŸ¥è¯¢æ¥å£"""
    try:
        db_query = text(
            "SELECT item_id, category, score FROM recommendation_results WHERE user_id = :uid AND model_type = 'RF-Optimized' ORDER BY `rank` ASC LIMIT :limit")
        results = pd.read_sql(db_query, engine, params={"uid": str(user_id), "limit": top_n})
        return results.to_dict(orient='records') if not results.empty else []
    except:
        return []