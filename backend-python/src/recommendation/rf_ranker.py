import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from src.database import engine
from sqlalchemy import text
import joblib
import os
import numpy as np
import gc
from concurrent.futures import ProcessPoolExecutor

# å…¨å±€å…±äº«å˜é‡ï¼Œå‡å°‘å­è¿›ç¨‹åºåˆ—åŒ–å¼€é”€
_shared_data = {}


def _init_worker(behavior_summary, user_cat_affinity, all_items_prepped, feature_names):
    """
    å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½é¢„å¤„ç†å¥½çš„ç‰¹å¾æ•°æ®
    """
    global _shared_data
    _shared_data['behavior_summary'] = behavior_summary
    _shared_data['user_cat_affinity'] = user_cat_affinity
    _shared_data['all_items_prepped'] = all_items_prepped
    _shared_data['feature_names'] = feature_names
    # é¢„åŠ è½½æ¨¡å‹åˆ°å†…å­˜
    _shared_data['model'] = joblib.load('libs/rf_model.pkl')


def _predict_user_batch_extreme_precision(user_batch, top_n=5, threshold=0.6):
    """
    é«˜æ€§èƒ½é¢„æµ‹å‡½æ•°ï¼šå‰”é™¤é‡å¤çš„ç‹¬çƒ­ç¼–ç é€»è¾‘
    """
    global _shared_data
    try:
        rf = _shared_data['model']
        # all_items_prepped å·²ç»æ˜¯åŒ…å« dummy å˜é‡çš„å®Œæ•´å•†å“è¡¨
        all_items = _shared_data['all_items_prepped']
        behavior_summary = _shared_data['behavior_summary']
        user_cat_affinity = _shared_data['user_cat_affinity']
        feature_names = _shared_data['feature_names']

        # 1. æ„é€ å€™é€‰é›† (ç¬›å¡å°”ç§¯) - ä¼˜åŒ–ç‚¹ï¼šåˆ©ç”¨é¢„ç¼–ç æ•°æ®
        combined = user_batch.assign(key=1).merge(all_items.assign(key=1), on='key').drop('key', axis=1)

        # 2. å¿«é€Ÿåˆå¹¶äº¤äº’ç‰¹å¾ä¸åå¥½ç‰¹å¾
        combined = combined.merge(behavior_summary, on=['user_id', 'item_id'], how='left')
        combined = combined.merge(user_cat_affinity, on=['user_id', 'category'], how='left')

        # 3. å¿«é€Ÿå¡«å……ç¼ºå¤±å€¼
        fill_cols = ['pv_count', 'add2cart', 'collect_num', 'like_num', 'cat_pref_score']
        combined[fill_cols] = combined[fill_cols].fillna(0)

        # 4. ç‰¹å¾å¯¹é½ï¼šè¡¥å…¨æ¨¡å‹éœ€è¦çš„åˆ—
        for col in feature_names:
            if col not in combined.columns:
                combined[col] = 0

        # 5. çŸ©é˜µåŒ–é¢„æµ‹
        X_pred = combined[list(feature_names)]
        combined['score'] = rf.predict_proba(X_pred)[:, 1]

        # 6. ç²¾å‡†è¿‡æ»¤ä¸åŠ¨æ€æˆªæ–­
        result = combined[combined['score'] >= threshold]
        result = result.sort_values(['user_id', 'score'], ascending=[True, False]).groupby('user_id').head(top_n).copy()

        # ä¿åº•é€»è¾‘
        if result.empty:
            result = combined.sort_values(['user_id', 'score'], ascending=[True, False]).groupby('user_id').head(
                1).copy()

        result['model_type'] = 'RF-Optimized'
        result['rank'] = result.groupby('user_id').cumcount() + 1

        del combined, X_pred
        gc.collect()
        return result[['user_id', 'item_id', 'score', 'model_type', 'category', 'rank']]
    except Exception as e:
        print(f"å­è¿›ç¨‹é¢„æµ‹æŠ¥é”™: {e}")
        return pd.DataFrame()


def train_recommendation_model(top_n=5, threshold=0.6):
    """
    ä¼˜åŒ–åçš„ä¸»è®­ç»ƒä¸å¹¶è¡Œé¢„æµ‹æµç¨‹
    """
    try:
        print("\n" + "========================================")
        print("ğŸš€ RF-Optimized é«˜æ€§èƒ½ç²¾å‡†æ¨¡å¼å¯åŠ¨")
        print("âš™ï¸  èµ„æºé™åˆ¶: 4 æ ¸å¿ƒå¹¶è¡Œ (CPU-Bound Optimization)")
        print(f"ğŸ“ ç­–ç•¥å‚æ•°ï¼šé˜ˆå€¼({threshold}) | Top-{top_n}")
        print("========================================")

        # 1. è®­ç»ƒæ•°æ®åŠ è½½
        query = """
                SELECT b.user_id, b.item_id, b.label, i.category,
                       COALESCE(b.pv_count, 0) as pv_count, COALESCE(b.add2cart, 0) as add2cart, 
                       COALESCE(b.collect_num, 0) as collect_num, COALESCE(b.like_num, 0) as like_num,
                       p.cluster_label, p.is_churn_risk, i.price, i.discount_rate, i.has_video
                FROM fact_user_behavior b
                JOIN usr_persona p ON b.user_id = p.user_id
                JOIN dim_item i ON b.item_id = i.item_id
                """
        df = pd.read_sql(query, engine)

        # è®¡ç®—ç±»ç›®åå¥½ç‰¹å¾
        user_cat_affinity = df.groupby(['user_id', 'category']).agg(cat_pref_score=('pv_count', 'sum')).reset_index()
        df = df.merge(user_cat_affinity, on=['user_id', 'category'], how='left')

        # 2. è®­ç»ƒé€»è¾‘ï¼šæ­£åˆ™åŒ–å¤„ç†
        print(">>> æ­£åœ¨æ‹Ÿåˆéšæœºæ£®æ—æ¨¡å‹ (n_estimators=150, max_depth=15)...")
        X_train = pd.get_dummies(df.drop(['label', 'user_id', 'item_id'], axis=1), columns=['category'])
        rf = RandomForestClassifier(
            n_estimators=150, max_depth=15, min_samples_leaf=10,
            class_weight='balanced', n_jobs=-1, random_state=42
        )
        rf.fit(X_train, df['label'])

        if not os.path.exists('libs'): os.makedirs('libs')
        joblib.dump(rf, 'libs/rf_model.pkl')
        feature_names = rf.feature_names_in_

        # 3. ã€æ ¸å¿ƒä¼˜åŒ–ç‚¹ã€‘ï¼šåœ¨ä¸»è¿›ç¨‹é¢„å…ˆå¤„ç†å•†å“ç‰¹å¾ç¼–ç 
        all_users = pd.read_sql("SELECT user_id, cluster_label, is_churn_risk FROM usr_persona", engine)
        all_items = pd.read_sql("SELECT item_id, price, discount_rate, has_video, category FROM dim_item", engine)

        # é¢„å…ˆç”Ÿæˆç‹¬çƒ­ç¼–ç ï¼Œé¿å…å­è¿›ç¨‹é‡å¤è®¡ç®—
        dummies = pd.get_dummies(all_items['category'], prefix='category')
        all_items_prepped = pd.concat([all_items, dummies], axis=1)

        behavior_summary = df[['user_id', 'item_id', 'pv_count', 'add2cart', 'collect_num', 'like_num']]
        active_users = all_users[all_users['user_id'].isin(df['user_id'].unique())]

        # ä»»åŠ¡åˆ†ç‰‡
        user_chunks = np.array_split(active_users, 20)
        predictions = []

        print(f">>> å¼€å§‹å¹¶è¡Œé¢„æµ‹ï¼Œåˆ†ç‰‡æ•°: 20")
        num_chunks = len(user_chunks)
        with ProcessPoolExecutor(
                max_workers=4,
                initializer=_init_worker,
                initargs=(behavior_summary, user_cat_affinity, all_items_prepped, feature_names)
        ) as executor:
            futures = [executor.submit(_predict_user_batch_extreme_precision, chunk, top_n, threshold) for chunk in
                       user_chunks]
            for i, f in enumerate(futures):
                res = f.result()
                if not res.empty: predictions.extend(res.to_dict(orient='records'))
                progress = (i + 1) / num_chunks * 100
                print(f"ğŸ“Š é¢„æµ‹è¿›åº¦: {progress:.0f}%")

        # 4. ä¼˜åŒ–åçš„æ•°æ®åº“å†™å…¥
        if predictions:
            res_df = pd.DataFrame(predictions)
            with engine.begin() as conn:
                conn.execute(text("DELETE FROM recommendation_results WHERE model_type = 'RF-Optimized'"))
                # ä½¿ç”¨ method='multi' å¤§å¹…æå‡æ’å…¥é€Ÿåº¦
                res_df.to_sql(
                    'recommendation_results',
                    con=conn,
                    if_exists='append',
                    index=False,
                    method='multi',
                    chunksize=2000
                )

        print(f"âœ… æ‰§è¡Œå®Œæ¯•ã€‚Threshold {threshold}, Top-{top_n}, å…±ç”Ÿæˆ {len(predictions)} æ¡æ•°æ®ã€‚")
        return True, "Success"
    except Exception as e:
        print(f"âŒ è¿è¡Œå¼‚å¸¸: {e}")
        return False, str(e)


def get_top_recommendations(user_id, top_n=5):
    """æŸ¥è¯¢æ¥å£"""
    try:
        db_query = text(
            "SELECT item_id, category, score FROM recommendation_results WHERE user_id = :uid AND model_type = 'RF-Optimized' ORDER BY `rank` ASC LIMIT :limit")
        results = pd.read_sql(db_query, engine, params={"uid": str(user_id), "limit": top_n})
        return results.to_dict(orient='records') if not results.empty else []
    except:
        return []